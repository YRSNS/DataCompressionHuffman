{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sn2R7p5NYzap",
        "CJFH81zJlqgU",
        "s08Jb6bEk_1B",
        "i-lVmO6RlQ_X"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilberever100/DataCompressionHuffman/blob/main/DataCompression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzAlUqniY-Cn"
      },
      "source": [
        "# File creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nVDU5WzY_uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2bce7d-2066-4238-d5a9-c1968d54c84c"
      },
      "source": [
        "# import enum\n",
        "# import os\n",
        "# from string import punctuation\n",
        "# import random\n",
        "\n",
        "# from numpy import character\n",
        "\n",
        "# words = \"qwertyuiopasdfghjklzxcvbnm \"  # + punctuation\n",
        "# print(words)\n",
        "# # Generate random file\n",
        "# fileGroup = {'small': [150, 300, 500, 700, 1000],  # smallFiles\n",
        "#              'medium': [2e+4, 3e+4, 5e+4, 7e+4, 1e+5],  # mediumFiles\n",
        "#              'large': [2e+7, 3e+7, 5e+7, 7e+7, 1e+8]}  # BigFiles\n",
        "\n",
        "# for key in fileGroup:\n",
        "#     for idx, fileSize in enumerate(fileGroup[key]):\n",
        "#         fileName = \"/content/\" + key + \"text\" + str(idx) + \".txt\"\n",
        "#         # Remove original file if exists\n",
        "#         if os.path.isfile(fileName):\n",
        "#             os.remove(fileName)\n",
        "\n",
        "#         # print(\"int(fileSize)\", int(fileSize))\n",
        "#         # Creates random generator file\n",
        "#         with open(fileName, \"w+\") as txtfile:\n",
        "#             strGen = ''.join(random.choices(words, k=int(fileSize)))\n",
        "#             txtfile.write(strGen)\n",
        "#             if key == \"small\":\n",
        "#                 print(fileName, \"file created:\",\n",
        "#                       os.path.getsize(fileName), \"bytes\")\n",
        "#             elif key == \"medium\":\n",
        "#                 print(fileName, \"file created:\",\n",
        "#                       os.path.getsize(fileName) / 1e3, \"KB\")\n",
        "#             else:\n",
        "#                 print(fileName, \"file created:\",\n",
        "#                       os.path.getsize(fileName) / 1e6, \"MB\")\n",
        "#                 # print(fileName, \"file created:\", os.path.getsize(fileName), \"bytes\")\n",
        "\n",
        "# # print(os.path.getsize('smalltext0.txt'), \"bytes\")\n",
        "# # print(os.path.getsize('smalltext1.txt'), \"bytes\")\n",
        "# # print(os.path.getsize('smalltext2.txt'), \"bytes\")\n",
        "# # print(os.path.getsize('smalltext3.txt'), \"bytes\")\n",
        "# # print(os.path.getsize('smalltext4.txt'), \"bytes\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwertyuiopasdfghjklzxcvbnm \n",
            "/content/smalltext0.txt file created: 0 bytes\n",
            "/content/smalltext1.txt file created: 0 bytes\n",
            "/content/smalltext2.txt file created: 0 bytes\n",
            "/content/smalltext3.txt file created: 0 bytes\n",
            "/content/smalltext4.txt file created: 0 bytes\n",
            "/content/mediumtext0.txt file created: 20.0 KB\n",
            "/content/mediumtext1.txt file created: 30.0 KB\n",
            "/content/mediumtext2.txt file created: 50.0 KB\n",
            "/content/mediumtext3.txt file created: 70.0 KB\n",
            "/content/mediumtext4.txt file created: 100.0 KB\n",
            "/content/largetext0.txt file created: 20.0 MB\n",
            "/content/largetext1.txt file created: 30.0 MB\n",
            "/content/largetext2.txt file created: 50.0 MB\n",
            "/content/largetext3.txt file created: 70.0 MB\n",
            "/content/largetext4.txt file created: 100.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn2R7p5NYzap"
      },
      "source": [
        "# Huffman"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap8b7M1nY1LR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2060ef-9685-4345-aa59-e6afd4ac4993"
      },
      "source": [
        "# A Huffman Tree Node\n",
        "import re\n",
        "from string import punctuation\n",
        "import time\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prob, symbol, left=None, right=None):\n",
        "\n",
        "        # probability of symbol\n",
        "        self.prob = prob\n",
        "\n",
        "        # symbol\n",
        "        self.symbol = symbol\n",
        "\n",
        "        # left node\n",
        "        self.left = left\n",
        "\n",
        "        # right node\n",
        "        self.right = right\n",
        "\n",
        "        # tree direction (0/1)\n",
        "        self.code = ''\n",
        "\n",
        "\n",
        "\"\"\" A helper function to print the codes of symbols by traveling Huffman Tree\"\"\"\n",
        "# codes = dict()\n",
        "\n",
        "#maxPercentage, currentPercentage, total_words = 0, 0, 0\n",
        "\n",
        "\n",
        "class HuffmanCompression:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.codes = dict()\n",
        "\n",
        "    def Calculate_Codes(self, node, val=''):\n",
        "        # huffman code for current node\n",
        "        newVal = val + str(node.code)\n",
        "\n",
        "        if(node.left):\n",
        "            self.Calculate_Codes(node.left, newVal)\n",
        "        if(node.right):\n",
        "            self.Calculate_Codes(node.right, newVal)\n",
        "\n",
        "        if(not node.left and not node.right):\n",
        "            prob = (1 / self.total_words) * 100  # 10\n",
        "            self.currentPercentage += prob\n",
        "\n",
        "            if self.maxPercentage >= round(self.currentPercentage, 2):\n",
        "                self.codes[node.symbol] = newVal  # 0100110\n",
        "            else:\n",
        "                self.codes[node.symbol] = node.symbol  # fox\n",
        "\n",
        "        return self.codes\n",
        "\n",
        "    \"\"\" A helper function to calculate the probabilities of symbols in given data\"\"\"\n",
        "\n",
        "    def Calculate_Probability(self, data):\n",
        "        symbols = dict()\n",
        "        # Separar por espacios\n",
        "        words = re.findall(r\"\\w+|[^\\w\\s]\", data, re.UNICODE)\n",
        "        punc = []  # value, index           Símbolos especiales\n",
        "\n",
        "        # matrix = [[\"?\", 1], [\"(\", 4]]\n",
        "        for idx, word in enumerate(words):\n",
        "            # for word,i in words:\n",
        "            if word in set(punctuation):\n",
        "                punc.append([word, idx])\n",
        "                continue\n",
        "\n",
        "            if symbols.get(word) == None:\n",
        "                symbols[word] = 1\n",
        "            else:\n",
        "                symbols[word] += 1\n",
        "\n",
        "        self.total_words = len(symbols)\n",
        "        return symbols, words, punc\n",
        "\n",
        "    def isUniqueOnKeys(self, word, keys, idx):\n",
        "        fullMatch = False  # Boolean to check for full word match\n",
        "\n",
        "        for key in keys:\n",
        "            if word == key and len(word) == len(key):\n",
        "                if len(self.data) != idx + 1 and (self.data[idx + 1] in [\" \", \"\\n\"] or self.data[idx + 1] in set(punctuation)):\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    \"\"\" A helper function to obtain the encoded output\"\"\"\n",
        "\n",
        "    def Output_Encoded(self, words, coding, punc):\n",
        "        huffmanEncoded = \"\"\n",
        "\n",
        "        keys = list(coding.keys())\n",
        "\n",
        "        # for word in words:\n",
        "        jointWord = \"\"\n",
        "        for idx, c in enumerate(self.data):\n",
        "            jointWord += c\n",
        "\n",
        "            if self.isUniqueOnKeys(jointWord, keys, idx):\n",
        "                huffmanEncoded += coding[jointWord]\n",
        "                jointWord = \"\"\n",
        "            elif jointWord in set(punctuation):\n",
        "                huffmanEncoded += jointWord\n",
        "                jointWord = \"\"\n",
        "            elif jointWord == \" \" or jointWord == \"\\n\":\n",
        "                huffmanEncoded += jointWord\n",
        "                jointWord = \"\"\n",
        "\n",
        "        return huffmanEncoded\n",
        "\n",
        "    \"\"\" A helper function to calculate the space difference between compressed and non compressed data\"\"\"\n",
        "\n",
        "    def Total_Gain(self, data, coding):\n",
        "        # total bit space to stor the data before compression\n",
        "        self.before_compression = len(data) * 8\n",
        "        self.after_compression = 0\n",
        "        symbols = coding.keys()\n",
        "        for symbol in symbols:\n",
        "            count = data.count(symbol)\n",
        "            # calculate how many bit is required for that symbol in total\n",
        "            self.after_compression += count * len(coding[symbol])\n",
        "        # print(\"Space usage before compression (in bits):\", before_compression)\n",
        "        # print(\"Space usage after compression (in bits):\",  after_compression)\n",
        "        # print()\n",
        "\n",
        "    def Huffman_Encoding(self, data, percentage=100):\n",
        "        startTime = time.time()\n",
        "        self.data = data\n",
        "        self.maxPercentage, self.currentPercentage = percentage, 0\n",
        "        symbol_with_probs, words, punc = self.Calculate_Probability(data)\n",
        "\n",
        "        #print(\"SYMBOL WITH PROBS\")\n",
        "        # print(symbol_with_probs)\n",
        "        # print()\n",
        "        symbols = symbol_with_probs.keys()\n",
        "        probabilities = symbol_with_probs.values()\n",
        "        #print(\"SYMBOLS: \", symbols)\n",
        "        #print(\"PROBABILITIES: \", probabilities)\n",
        "        # print()\n",
        "\n",
        "        nodes = []\n",
        "\n",
        "        # converting symbols and probabilities into huffman tree nodes\n",
        "        for symbol in symbols:\n",
        "            nodes.append(Node(symbol_with_probs.get(symbol), symbol))\n",
        "\n",
        "        while len(nodes) > 1:\n",
        "            # sort all the nodes in ascending order based on their probability\n",
        "            nodes = sorted(nodes, key=lambda x: x.prob)\n",
        "            # for node in nodes:\n",
        "            #      print(node.symbol, node.prob)\n",
        "\n",
        "            # pick 2 smallest nodes\n",
        "            right = nodes[0]\n",
        "            left = nodes[1]\n",
        "\n",
        "            left.code = 0\n",
        "            right.code = 1\n",
        "\n",
        "            # combine the 2 smallest nodes to create new node\n",
        "            newNode = Node(left.prob+right.prob, left.symbol +\n",
        "                           right.symbol, left, right)\n",
        "\n",
        "            nodes.remove(left)\n",
        "            nodes.remove(right)\n",
        "            nodes.append(newNode)\n",
        "        # def Calculate_Codes(node, total_words, maxPercentage, currentPercentage, val=''):\n",
        "\n",
        "        huffman_encoding = self.Calculate_Codes(nodes[0])\n",
        "        #print(\"symbols with codes\", huffman_encoding)\n",
        "        # Gain of compressed vs decompressed\n",
        "        \n",
        "        self.Total_Gain(data, huffman_encoding)\n",
        "        encoded_output = self.Output_Encoded(words, huffman_encoding, punc)\n",
        "        endTime = time.time()\n",
        "        return encoded_output, nodes[0], endTime - startTime\n",
        "\n",
        "    def find_code(self, word, huffman_tree):\n",
        "        tree_head = huffman_tree\n",
        "        decoded_output = []\n",
        "        word_formed = \"\"\n",
        "        for x in word:\n",
        "            if x == '1':\n",
        "                word_formed += \"1\"\n",
        "                huffman_tree = huffman_tree.right\n",
        "            elif x == '0':\n",
        "                word_formed += \"0\"\n",
        "                huffman_tree = huffman_tree.left\n",
        "            try:\n",
        "                if huffman_tree.left.symbol == None and huffman_tree.right.symbol == None:\n",
        "                    pass\n",
        "            except AttributeError:\n",
        "                decoded_output.append(word_formed)\n",
        "                word_formed = \"\"\n",
        "                huffman_tree = tree_head\n",
        "\n",
        "        return decoded_output\n",
        "\n",
        "    def Huffman_Decoding(self, encoded_data, huffman_tree):\n",
        "        tree_head = huffman_tree\n",
        "        decoded_output = []\n",
        "        not_encoded_word = \"\"\n",
        "        for x in encoded_data:\n",
        "            if x == '1':\n",
        "                huffman_tree = huffman_tree.right\n",
        "            elif x == '0':\n",
        "                huffman_tree = huffman_tree.left\n",
        "            elif x in set(punctuation):  # Para añadir puntuaciones\n",
        "                if not_encoded_word:\n",
        "                    decoded_output.append(not_encoded_word)\n",
        "                    not_encoded_word = \"\"\n",
        "\n",
        "                decoded_output.append(x)\n",
        "            else:  # non encoded word\n",
        "                not_encoded_word += x\n",
        "\n",
        "            try:\n",
        "                if huffman_tree.left.symbol == None and huffman_tree.right.symbol == None:\n",
        "                    pass\n",
        "            except AttributeError:\n",
        "                if not_encoded_word:\n",
        "                    decoded_output.append(not_encoded_word)\n",
        "                    not_encoded_word = \"\"\n",
        "\n",
        "                decoded_output.append(huffman_tree.symbol)\n",
        "                huffman_tree = tree_head\n",
        "\n",
        "        string = ''.join([str(item) for item in decoded_output])\n",
        "        return string\n",
        "\n",
        "\n",
        "# Hi how are you?\n",
        "# Huffman: 01 111 00 110?\n",
        "# 01111001 = Y\n",
        "# 00000010 = d\n",
        "# CDC: Yd?\n",
        "\n",
        "\"\"\" First Test \"\"\"\n",
        "# data = \"Hi how are you?\"\n",
        "# encoding, tree = Huffman_Encoding(data)\n",
        "# print(\"Encoded output:\", encoding)\n",
        "# print(\"Decoded Output:\", Huffman_Decoding(encoding, tree))\n",
        "\n",
        "\n",
        "\"\"\" Second Test \"\"\"\n",
        "\n",
        "# f = open(\"demofile.txt\", \"r\")\n",
        "\n",
        "# huff = HuffmanCompression()\n",
        "\n",
        "# data = f.read()\n",
        "# print(data)\n",
        "# encoding, tree = huff.Huffman_Encoding(data)\n",
        "# print(\"Encoded output:\", encoding)\n",
        "# print(\"Decoded Output:\", huff.Huffman_Decoding(encoding, tree))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Second Test '"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfJ_unOCY1T_"
      },
      "source": [
        "# CDC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFH81zJlqgU"
      },
      "source": [
        "## Libraries and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRam6QbHlql-"
      },
      "source": [
        "from re import sub\n",
        "import os\n",
        "import time\n",
        "\n",
        "huffmanObj = HuffmanCompression()\n",
        "punc_positions = []\n",
        "symbols_positions = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s08Jb6bEk_1B"
      },
      "source": [
        "## Decompression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MScLn5XJlCEs"
      },
      "source": [
        "\n",
        "def decompression(encoded, symbols, tree):\n",
        "    # print(\"########################################\")\n",
        "    # print(\"Decompression\")\n",
        "    # Part 1: Decoding CDC\n",
        "    cdcDecoded = \"\"\n",
        "    keys = list(symbols.keys())  # lista de binarios [01100110, ...]\n",
        "    values = list(symbols.values())  # [hexa, count]\n",
        "    sym = []\n",
        "    for en in values:\n",
        "        sym.append(en[0])\n",
        "\n",
        "    checkForWord = \"\"\n",
        "    for idxEncoded, c in enumerate(encoded):\n",
        "        if c == \"\\n\":\n",
        "            cdcDecoded += '\\n'\n",
        "            continue\n",
        "        if c == \" \":\n",
        "            cdcDecoded += \" \"\n",
        "            continue\n",
        "        checkForWord += c\n",
        "\n",
        "        # a la hora comprimir\n",
        "        # guardar posiciones\n",
        "\n",
        "        # hola como 0001 1000stas      0001 1000\n",
        "        if checkForWord in set(punctuation):\n",
        "            if idxEncoded in punc_positions:\n",
        "                cdcDecoded += checkForWord\n",
        "                checkForWord = \"\"\n",
        "            else:\n",
        "                # for por cada valor codificado\n",
        "                for idxValues, en in enumerate(values):\n",
        "                    if checkForWord == en[0]:  # match de word codificado\n",
        "                        # Ü = [\"1101\", \"1100\"]\n",
        "                        huffmanEncoded = huffmanObj.find_code(\n",
        "                            keys[idxValues], tree)\n",
        "\n",
        "                        # \"Word padded with zeroes, removing...\n",
        "                        prov = en[1]\n",
        "                        if en[1] == 0:\n",
        "                            prov = 1\n",
        "                        while len(huffmanEncoded) > prov:  # en[1] == count\n",
        "                            huffmanEncoded.pop(0)\n",
        "\n",
        "                        if en[1] == 0:\n",
        "                            # une mediante espacios el huffmanEncoded\n",
        "                            cdcDecoded += ''.join([str(item)\n",
        "                                                   for item in huffmanEncoded])\n",
        "                        else:\n",
        "                            cdcDecoded += ' '.join([str(item)\n",
        "                                                    for item in huffmanEncoded])\n",
        "\n",
        "                        # añadir un espacio si existe dos palabras comprimidas juntas\n",
        "                        # if (encoded[idxEncoded + 1] != \" \" or encoded[idxEncoded + 1] != \"\\n\") and not encoded[idxEncoded + 1] in set(punctuation):\n",
        "                        #     cdcDecoded = cdcDecoded + \" \"\n",
        "                        checkForWord = \"\"\n",
        "                        break\n",
        "\n",
        "        else:\n",
        "            # for por cada valor codificado\n",
        "            found = False\n",
        "            for idxValues, en in enumerate(values):\n",
        "\n",
        "                # Si la palabra comprimida no coincide, pegar tal cual\n",
        "                if not idxEncoded in symbols_positions:\n",
        "                    cdcDecoded += checkForWord\n",
        "                    checkForWord = \"\"\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "                if checkForWord == en[0]:  # match de word codificado\n",
        "\n",
        "                    # Ü = [\"1101\", \"1100\"]\n",
        "                    huffmanEncoded = huffmanObj.find_code(\n",
        "                        keys[idxValues], tree)\n",
        "\n",
        "                    # \"Word padded with zeroes, removing...\n",
        "                    while len(huffmanEncoded) > en[1]:  # en[1] == count\n",
        "                        huffmanEncoded.pop(0)\n",
        "\n",
        "                    # une mediante espacios el huffmanEncoded\n",
        "                    cdcDecoded += ' '.join([str(item)\n",
        "                                           for item in huffmanEncoded])\n",
        "                    # añadir un espacio si existe dos palabras comprimidas juntas\n",
        "# 'Ü \\x0b, © \\x87 e\\nC ! \\x07. :)\\n:('\n",
        "\n",
        "                    checkForWord = \"\"\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                cdcDecoded += checkForWord\n",
        "                checkForWord = \"\"\n",
        "\n",
        "    # print(\"cdcDecoded:\", cdcDecoded)\n",
        "\n",
        "    # Part 2: Decoding Huffman\n",
        "\n",
        "    huffmanDecoded = huffmanObj.Huffman_Decoding(cdcDecoded, tree)\n",
        "\n",
        "    return huffmanDecoded\n",
        "\n",
        "    # print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGuJQetJlFIC"
      },
      "source": [
        "## Compression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "007StOrOY2FD"
      },
      "source": [
        "def partial_compression(data=\"Hi, how (are)- you?\", percentage=90):\n",
        "    # Huffman con percentage\n",
        "    symbols = dict()\n",
        "\n",
        "    # 11,10.01-00?\n",
        "    text_encoded, tree, _ = huffmanObj.Huffman_Encoding(\n",
        "        data, percentage)\n",
        "\n",
        "    count = 0  # Contador para el numero de palabras\n",
        "    partialEncoded = \"\"  # Para verificar si es comprimida\n",
        "    cdcEncoding = \"\"  # Parte comprimida\n",
        "    checkForWord = \"\"  # Para ir avanzando por el texto\n",
        "    # punc_positions = []  # Posiciones de las puntuaciones\n",
        "    for idx, c in enumerate(text_encoded):\n",
        "        if c == \" \":\n",
        "            continue\n",
        "\n",
        "        checkForWord += c\n",
        "        partialEncoded += c\n",
        "\n",
        "        # Verificar si es palabra comprimida\n",
        "        if c == \"0\" or c == \"1\":\n",
        "            # si es palabra comprimida y luego tiene 0s 1s   -> \"hola 0101\"\n",
        "            if len(checkForWord) >= 2:\n",
        "                if checkForWord[-2] != \"0\" and checkForWord[-2] != \"1\":\n",
        "                    cdcEncoding += checkForWord[:-1]\n",
        "                    checkForWord = c\n",
        "                    partialEncoded = c\n",
        "\n",
        "            # Contador para verificar al descomprimir cuantos\n",
        "            if idx + 1 != len(text_encoded) and not text_encoded[idx + 1] in [\"0\", \"1\"] and isEncodedWord(partialEncoded):\n",
        "                count += 1\n",
        "\n",
        "                partialEncoded = \"\"\n",
        "\n",
        "            # encode checkForWord\n",
        "            if len(checkForWord) == 8:\n",
        "                binary = checkForWord.zfill(8)  # rellenar con 0s\n",
        "                x = chr(int(binary, 2))\n",
        "                symbols_positions.append(len(cdcEncoding))\n",
        "\n",
        "                cdcEncoding += x  # guarda los encodings cuando llegan a 8 bits\n",
        "                if text_encoded[idx + 1] == \" \":\n",
        "                    cdcEncoding += \" \"\n",
        "\n",
        "                symbols[binary] = [x, count, 8 - len(checkForWord)]\n",
        "                count = 0\n",
        "                checkForWord = \"\"\n",
        "                partialEncoded = \"\"\n",
        "\n",
        "      # si encuentra puntuacion\n",
        "      # verificar el indice en algun simbolo\n",
        "        elif c in set(punctuation) or c == '\\n':\n",
        "            if len(checkForWord) >= 2:\n",
        "                if checkForWord[-2] != \"0\" and checkForWord[-2] != \"1\":\n",
        "                    checkForWord = checkForWord[:-1]\n",
        "                    cdcEncoding += checkForWord\n",
        "                    checkForWord = \"\"\n",
        "                    partialEncoded = \"\"\n",
        "                else:\n",
        "                    checkForWord = checkForWord[:-1]\n",
        "                    binary = checkForWord.zfill(8)\n",
        "                    x = chr(int(binary, 2))  # 000000Hi\n",
        "                    # add to CDC symbols\n",
        "                    symbols[checkForWord] = [\n",
        "                        x, count, 8 - len(checkForWord)]\n",
        "\n",
        "                    checkForWord = \"\"\n",
        "                    partialEncoded = \"\"\n",
        "                    count = 0\n",
        "\n",
        "                    symbols_positions.append(len(cdcEncoding))\n",
        "                    cdcEncoding += x\n",
        "                    if text_encoded[idx - 1] == \" \":\n",
        "                        cdcEncoding += \" \"\n",
        "\n",
        "            punc_positions.append(len(cdcEncoding))\n",
        "            cdcEncoding = cdcEncoding + c\n",
        "            if idx + 1 != len(text_encoded) and text_encoded[idx + 1] == \" \":\n",
        "                cdcEncoding += \" \"\n",
        "\n",
        "            checkForWord = \"\"\n",
        "            partialEncoded = \"\"\n",
        "\n",
        "        else:  # Ejm: 00011 hola\n",
        "            if len(checkForWord) >= 2: # Ejm: 00011 hola\n",
        "                # palabra no comprimida\n",
        "                # Ejm: ho1\n",
        "                if checkForWord[-2] != \"0\" and checkForWord[-2] != \"1\":  # si es letra u otro\n",
        "                    cdcEncoding += checkForWord\n",
        "                    checkForWord = \"\"\n",
        "                    partialEncoded = \"\"\n",
        "                    if idx + 1 != len(text_encoded) and text_encoded[idx + 1] == \" \":\n",
        "                        cdcEncoding += \" \"\n",
        "\n",
        "                else:  # palabra comprimida Ejm: \"0101h\"\n",
        "                    checkForWord = checkForWord[:-1]\n",
        "                    binary = checkForWord.zfill(8)  # rellenar con 0s\n",
        "                    x = chr(int(binary, 2))\n",
        "                    symbols_positions.append(len(cdcEncoding))\n",
        "                    cdcEncoding += x  # guarda los encodings cuando llegan a 8 bits\n",
        "                    partialEncoded = \"\"\n",
        "                    if idx - 1 != len(text_encoded) and text_encoded[idx - 1] == \" \":\n",
        "                        cdcEncoding += \" \"\n",
        "\n",
        "                    symbols[checkForWord] = [\n",
        "                        x, count, 8 - len(checkForWord)]\n",
        "                    count = 0\n",
        "                    checkForWord = c\n",
        "                    partialEncoded = c\n",
        "            else:  # copiar palabra no comprimida\n",
        "                cdcEncoding += checkForWord\n",
        "                checkForWord = \"\"\n",
        "                partialEncoded = \"\"\n",
        "\n",
        "                if idx + 1 != len(text_encoded) and text_encoded[idx + 1] == \" \":\n",
        "                    cdcEncoding += \" \"\n",
        "\n",
        "    if len(checkForWord):\n",
        "        binary = checkForWord.zfill(8)  # rellenar con 0s\n",
        "\n",
        "        x = chr(int(binary, 2))\n",
        "        symbols_positions.append(len(cdcEncoding))\n",
        "        cdcEncoding += x  # guarda los encodings cuando llegan a 8 bits\n",
        "\n",
        "        # add to CDC symbols      de la forma   011 10 010=k  =>      [k, 3]\n",
        "        symbols[checkForWord] = [x, count, 8 - len(checkForWord)]\n",
        "        checkForWord = \"\"\n",
        "\n",
        "    return cdcEncoding, symbols, tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-lVmO6RlQ_X"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e8A8VnIlQ5r"
      },
      "source": [
        "def isEncodedWord(word):\n",
        "    isInValues = word in huffmanObj.codes.values()     # es una palabra sin codificar?\n",
        "    # no es una palabra dentro del diccionario?\n",
        "    isNotInKeys = not word in huffmanObj.codes\n",
        "\n",
        "    # no es un signo de puntuación?\n",
        "    isNotPuntuation = not word in set(punctuation)\n",
        "\n",
        "    return (isInValues and isNotInKeys) and isNotPuntuation\n",
        "    \n",
        "def Total_Gain(data, coding):\n",
        "    # total bit space to store the data before compression\n",
        "    before_compression = len(data) * 8\n",
        "    after_compression = len(coding) * 8\n",
        "    return before_compression, after_compression\n",
        "\n",
        "\n",
        "def execute(data, percentage=100):\n",
        "    startEncoding = time.time()\n",
        "    cdcEncoding, symbols, tree = partial_compression(\n",
        "        data, percentage=percentage)  # Hi,how.01-00?\n",
        "    endEncoding = time.time()\n",
        "\n",
        "    # huffmanDecoded = decompression(cdcEncoding, symbols, tree)\n",
        "\n",
        "    # sameCounter = 0\n",
        "    # same = True\n",
        "    # for i in range(len(data)):\n",
        "    #     if huffmanDecoded[i] != data[i]:\n",
        "    #         sameCounter += 1\n",
        "    #         print(data[i],\"--\",huffmanDecoded[i],\"--pos: \", i)\n",
        "    #         same = False\n",
        "    #         if sameCounter == 30:\n",
        "    #             break\n",
        "\n",
        "    before, after = Total_Gain(data, cdcEncoding)\n",
        "\n",
        "    compression_rate = after / before\n",
        "\n",
        "    return cdcEncoding, compression_rate, endEncoding - startEncoding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrKTYaSAY2o8"
      },
      "source": [
        "# Plot graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j9pR1ZpnWeS"
      },
      "source": [
        "## Plot def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAz7xqCqY2Jg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def plotGraphs(original_sizes, reduced_sizes, percentage):\n",
        "    n_groups = len(original_sizes)\n",
        "    # create plot\n",
        "    fig, ax = plt.subplots()\n",
        "    index = np.arange(n_groups)\n",
        "    bar_width = 0.2\n",
        "    opacity = 0.8\n",
        "\n",
        "    rects1 = plt.bar(index, original_sizes, bar_width,\n",
        "                     alpha=opacity,\n",
        "                     color='b',\n",
        "                     label='Original Sizes')\n",
        "\n",
        "    rects2 = plt.bar(index + bar_width, reduced_sizes, bar_width,\n",
        "                     alpha=opacity,\n",
        "                     color='g',\n",
        "                     label='Reduced Sizes')\n",
        "\n",
        "    # rects3 = plt.bar(index + 2*bar_width, reduced_sizes, bar_width,\n",
        "    # alpha=opacity,\n",
        "    # color='r',\n",
        "    # label='Reduced Sizes')\n",
        "\n",
        "    plt.xlabel('Files')\n",
        "    plt.ylabel('Bytes')\n",
        "    plt.title('Bytes by File')\n",
        "    plt.xticks(index + bar_width, ('A', 'B', 'C', 'D'))\n",
        "    plt.xticks(index + bar_width, percentage)\n",
        "    # plt.xticks(index + bar_width, ('A','B'))\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plotGraphs2(reduced_sizes, percentage, labels):\n",
        "    # reduced_sizes: array of numpy arrays(every element has an array of size of a reduced file)\n",
        "    reduced_sizes = np.array(reduced_sizes)\n",
        "    n_groups = len(reduced_sizes)\n",
        "    # create plot\n",
        "    fig, ax = plt.subplots()\n",
        "    index = np.arange(n_groups)\n",
        "    bar_width = 0.1\n",
        "    opacity = 0.8\n",
        "\n",
        "    colors = ['#4f81bc', '#c0504d', '#9cba5b', '#8063a1', '#4aacc5']\n",
        "\n",
        "    print(\"len(reduced_sizes)   : \", len(reduced_sizes))\n",
        "    for idx2 in range(len(reduced_sizes)):\n",
        "        # print(\"SHAPE: \", reduced_sizes[idx2].shape)\n",
        "        # print(\"ELEMENTS: \", reduced_sizes[idx2])\n",
        "\n",
        "        plt.bar(index + idx2 * bar_width, reduced_sizes[idx2], bar_width,\n",
        "                alpha=opacity,\n",
        "                color=colors[idx2],\n",
        "                label=labels[idx2])\n",
        "\n",
        "    plt.xlabel('Files')\n",
        "    plt.ylabel('Compression Rate')\n",
        "    if key == \"small\":\n",
        "        plt.title(\n",
        "            'Comparison of Compression Ratio of Different Algorithms (Very Small Size File)')\n",
        "    elif key == \"medium\":\n",
        "        plt.title(\n",
        "            'Comparison of Compression Ratio of Different Algorithms (Small Size File)')\n",
        "    else:\n",
        "        plt.title(\n",
        "            'Comparison of Compression Ratio of Different Algorithms (Large Small Size File)')\n",
        "\n",
        "    plt.xticks(index + bar_width, percentage)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVBsn4WnYm3"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg16CRmenUC2",
        "outputId": "e63e60ac-692a-40b8-9ca3-bc513c8a099a"
      },
      "source": [
        "\n",
        "# call execute on each file\n",
        "compressions = [50, 70, 90, 100]\n",
        "\n",
        "# Table 1 data\n",
        "compression_rate = []\n",
        "encodingTimes = []\n",
        "file_data = []\n",
        "labels = []\n",
        "\n",
        "key = \"medium\"\n",
        "\n",
        "if key == \"small\":\n",
        "    sizeTerm = \"Bytes\"\n",
        "elif key == \"medium\":\n",
        "    sizeTerm = \"KB\"\n",
        "else:\n",
        "    sizeTerm = \"MB\"\n",
        "\n",
        "\n",
        "compression_rate_general = []\n",
        "compression_rate = []\n",
        "\n",
        "for i in range(5):\n",
        "    fileName = \"/content/\" + key + \"text\" + str(i) + \".txt\"\n",
        "    f = open(fileName, \"r\")\n",
        "    data = f.read()\n",
        "    reduced = []\n",
        "    compreoriginal = []\n",
        "\n",
        "    print(\"Running file\", i+1)\n",
        "\n",
        "    # Add HUFFMAN\n",
        "    text_encoded, tree, encodingTime = huffmanObj.Huffman_Encoding(data)\n",
        "    compression_rate.append(float(\n",
        "        huffmanObj.after_compression) / float(huffmanObj.before_compression))\n",
        "\n",
        "    compression_rate_general.append(float(\n",
        "        huffmanObj.after_compression) / float(huffmanObj.before_compression))\n",
        "\n",
        "    encodingTimes.append(encodingTime)\n",
        "\n",
        "    compreoriginal.append(huffmanObj.before_compression)\n",
        "    reduced.append(huffmanObj.after_compression)\n",
        "\n",
        "print(encodingTimes)\n",
        "\n",
        "file_data.append(compression_rate)\n",
        "\n",
        "for idx, c in enumerate(compressions):\n",
        "    reduced = []\n",
        "    compreoriginal = []\n",
        "    compression_rate = []\n",
        "    if key == \"small\":\n",
        "      fileSize = os.path.getsize(fileName)\n",
        "    if key == \"medium\":\n",
        "      fileSize = os.path.getsize(fileName) / 1e3\n",
        "    else:\n",
        "      fileSize = os.path.getsize(fileName) / 1e6\n",
        "    print(\"Compression testing: \" + str(c))\n",
        "    for i in range(5):\n",
        "        fileName = \"/content/\" + key + \"text\" + str(i) + \".txt\"\n",
        "        if i == 0:\n",
        "            labels.append(\"File original size=\" +\n",
        "                          str(round(fileSize,2)) + \" \" + sizeTerm)\n",
        "\n",
        "        f = open(fileName, \"r\")\n",
        "        data = f.read()\n",
        "\n",
        "        cdcEncoding, com_rate, encodingTime = execute(data, c)\n",
        "        print(\"File \"+str(i+1)+\": \", str(encodingTime)+\" s\")\n",
        "\n",
        "        # Data for table 1\n",
        "        compression_rate.append(com_rate)\n",
        "        compression_rate_general.append(com_rate)\n",
        "        encodingTimes.append(encodingTime)\n",
        "\n",
        "        # Data for bar plot\n",
        "        original_size, compressed_size = Total_Gain(data, cdcEncoding)\n",
        "        compreoriginal.append(original_size)\n",
        "        reduced.append(compressed_size)\n",
        "        # print(\"result: \",cdcEncoding,\"\\ncompression: \", com_rate)\n",
        "        punc_positions = []\n",
        "        symbols_positions = []\n",
        "\n",
        "    file_data.append(compression_rate)\n",
        "\n",
        "# Table 1\n",
        "for i in range(len(compressions)):\n",
        "    compressions[i] = \"CBC-\" + str(compressions[i]) + \"%\"\n",
        "compressions.insert(0, \"Huffman\")\n",
        "\n",
        "for idx in range(len(compressions)):\n",
        "    print(compressions[idx] + \":\",\n",
        "          round(encodingTimes[idx], 3), \"s\", \n",
        "          round(compression_rate_general[idx], 3))\n",
        "\n",
        "# Plot graphs\n",
        "\n",
        "plotGraphs2(file_data, compressions, labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running file 1\n",
            "Running file 2\n",
            "Running file 3\n",
            "Running file 4\n",
            "Running file 5\n",
            "[0.5835881233215332, 1.957244873046875, 6.231351375579834, 14.778237342834473, 33.72843384742737]\n",
            "Compression testing: 50\n",
            "File 1:  6.273981332778931 s\n",
            "File 2:  9.510699033737183 s\n",
            "File 3:  16.305938959121704 s\n",
            "File 4:  23.108334064483643 s\n",
            "File 5:  33.52502489089966 s\n",
            "Compression testing: 70\n",
            "File 1:  6.446605205535889 s\n",
            "File 2:  9.511423110961914 s\n",
            "File 3:  15.930874347686768 s\n",
            "File 4:  22.539212465286255 s\n",
            "File 5:  32.73789930343628 s\n",
            "Compression testing: 90\n",
            "File 1:  6.3272716999053955 s\n"
          ]
        }
      ]
    }
  ]
}